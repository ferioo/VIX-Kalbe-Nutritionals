# -*- coding: utf-8 -*-
"""x.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13zq-jBN4FV9P5hkocC3rTxu-Igj79TMi

# **Import Library**
"""

!pip install pmdarima

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from yellowbrick.cluster import KElbowVisualizer

import warnings
warnings.filterwarnings('ignore')

"""# **Import Data**"""

df_customer = pd.read_csv('Customer.csv', sep= ';')
df_product = pd.read_csv('Product.csv', sep= ';')
df_store = pd.read_csv('Store.csv', sep= ';')
df_transaction = pd.read_csv('transaction.csv', sep= ';')

"""# **Data Preparation**"""

df_transaction.info()

df_customer.info()

df_product.info()

df_store.info()

df1 = pd.merge(df_transaction, df_customer, on='CustomerID', how='inner')
df2 = pd.merge(df1, df_store, on = 'StoreID', how = 'inner')
df_akhir = pd.merge(df2, df_product, on = 'ProductID', how = 'inner')
df_akhir.head()

df_akhir.info()

(df_akhir['Price_x']/df_akhir['Price_y']).value_counts()

df_akhir.drop(columns ='Price_y', inplace = True)

df_akhir.isna().sum()

df_akhir.duplicated().sum()

#drop missing values karena tidak signifikan jumlahnya
df_akhir.dropna(inplace = True)
df_akhir.isna().sum()

df_akhir.info()

df_akhir.Date

"""**Change Data Type of Irrelevant Data Types**





"""

df_akhir['Date'] = pd.to_datetime(df_akhir['Date'])
df_akhir['Longitude'] = df_akhir['Longitude'].apply(lambda x: x.replace(',','.')).astype(float)
df_akhir['Latitude'] = df_akhir['Latitude'].apply(lambda x: x.replace(',','.')).astype(float)

df_akhir.head()

df_akhir.info()

"""# Time Series Regression Analysis"""

df_regression = df_akhir.groupby('Date').agg({'Qty':'sum'})
df_regression.plot(figsize=(12,4))

#Split Data Train & Data Test
print(df_regression.shape)
test_size = round(df_regression.shape[0] * 0.15)
train=df_regression.iloc[:-1*(test_size)]
test=df_regression.iloc[-1*(test_size):]
print(train.shape,test.shape)

plt.figure(figsize=(12,4))
sns.lineplot(data=train, x=train.index, y=train['Qty'])
sns.lineplot(data=test, color="green", x=test.index, y=test['Qty'])
plt.show()

"""# **Data Stationary Check**"""

def adf_test(dataset):
     df_test = adfuller(dataset, autolag = 'AIC')
     print("1. ADF : ",df_test[0])
     print("2. P-Value : ", df_test[1])
     print("3. Num Of Lags : ", df_test[2])
     print("4. Num Of Observations Used For ADF Regression:", df_test[3])
     print("5. Critical Values :")
     for key, val in df_test[4].items():
         print("\t",key, ": ", val)
adf_test(df_regression)

"""P-Value < 0.05 shows that the data is stationary and can be used in time series analysis with ARIMA"""

# ACF and PACF plots to determine p and q values
fig, ax = plt.subplots(1, 2, figsize=(12, 4))
plot_acf(df_regression.diff().dropna(), lags=40, ax=ax[0])
plot_pacf(df_regression.diff().dropna(), lags=40, ax=ax[1])
plt.show()

"""The Autocorrelation graph (ACF) shows that the p order is 2 because the first and second lag is significantly out of the significant limit,
meanwhile the Partial Autocorrelation graph (PCF) shows that the q order is 3 due to the significant correlation of the first until third lag.

# **Modelling**

Auto-fit ARIMA
"""

auto_arima = pm.auto_arima(train, stepwise=False, seasonal=False)
auto_arima

"""In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

Hyperparameter Tuning
"""

# Make p, d, q list
p = range(0, 4)  # 0-3
d = range(0, 4)  # 0-3
q = range(0, 4)  # 0-3
# make p, d, dan q combination using product function
pdq = list(product(p, d, q))
print(pdq)

# Membuat list untuk menyimpan Skor AIC
aic_scores = []
# Melakukan grid search manual untuk menemukan p, d, q yang optimal
for param in pdq:
    # Melakukan fitting ARIMA model
    model = ARIMA(df_regression, order=param)
    model_fit = model.fit()
    # Menambahkan aic score ke list
    aic_scores.append({'par': param, 'aic': model_fit.aic})

# Memilih aic score terkecil
best_aic = min(aic_scores, key=lambda x: x['aic'])
print(best_aic)

#Hyperparameter tuning
model_hyper = ARIMA(train, order=best_aic['par'])
model_fit_hyper = model_hyper.fit()

"""Manual Hyperparameter Tuning"""

#Trial and error tuning
model_manual = ARIMA(train, order=(40,2,2))
model_fit_manual = model_manual.fit()

"""Plot Forecasting"""

#plot forecasting
forecast_manual = model_fit_manual.forecast(len(test))
forecast_hyper = model_fit_hyper.forecast(len(test))
forecast_auto = auto_arima.predict(len(test))

df_plot = df_regression.iloc[-100:]

df_plot['forecast_manual'] = [None]*(len(df_plot)-len(forecast_manual)) + list(forecast_manual)
df_plot['forecast_hyper'] = [None]*(len(df_plot)-len(forecast_hyper)) + list(forecast_hyper)
df_plot['forecast_auto'] = [None]*(len(df_plot)-len(forecast_auto)) + list(forecast_auto)

df_plot.plot(figsize=(12, 4))
plt.show()

"""**Metrics Evaluation**"""

# Manual parameter tuning metrics
mae = mean_absolute_error(test, forecast_hyper)
mape = mean_absolute_percentage_error(test, forecast_hyper)
rmse = np.sqrt(mean_squared_error(test, forecast_hyper))

print(f'Mean Absolute Error - manual            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - manual : {round(mape,4)}')
print(f'Root Mean Square Error - manual         : {round(rmse,4)}')

#Auto-fit ARIMA metrics
mae = mean_absolute_error(test, forecast_auto)
mape = mean_absolute_percentage_error(test, forecast_auto)
rmse = np.sqrt(mean_squared_error(test, forecast_auto))

print(f'Mean Absolute Error - auto            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - auto : {round(mape,4)}')
print(f'Root Mean Square Error - auto         : {round(rmse,4)}')

"""Manual Hyperparameter Tuning shows the best evaluation metrics.

# Forecast Quantity Sales with The Best **Parameter**
"""

# Apply model to forecast data
model = ARIMA(df_regression, order=(40, 2, 2))
model_fit = model.fit()
forecast = model_fit.forecast(steps=31)

df_regression

forecast

#Plot forecasting
plt.figure(figsize=(12,4))
plt.plot(df_regression)
plt.plot(forecast,color='red')
plt.title('Quantity Sales Forecasting')
plt.show()

forecast.describe()

"""From the forecast, the average quantity sales in January 2023 is 44.489775 or up rounded to around 44 pcs/day.

# **Clustering**
"""

df_akhir.head()

df_preclust = df_akhir.groupby('CustomerID').agg({'TransactionID':'count',
                                                   'Qty':'sum',
                                                   'TotalAmount':'sum'}).reset_index()
df_preclust

df_preclust.info()

df_cluster = df_preclust.drop(columns = ['CustomerID'])
df_cluster.head()

df_cluster.info()

df_cluster.isna().sum()

#Standarisasi dataset
X = df_cluster.values
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data=X_std,columns=df_cluster.columns)
df_std.isna().sum()

#Normalisasi dataset dengan minmaxscaler
X_norm = MinMaxScaler().fit_transform(X)
X_norm

# Normalisasi dataset dengan preprocessing sklearn
X_norm2 = preprocessing.normalize(df_cluster)
X_norm2

X_std

df_std

wcss= []
for n in range (1,11):
    model1 = KMeans(n_clusters=n, init='k-means++', n_init = 10, max_iter=100, tol =0.0001, random_state = 100)
    model1.fit(X_std)
    wcss.append(model1.inertia_)
print(wcss)

plt.figure(figsize=(8,6))
plt.plot(list(range(1,11)), wcss, color = 'blue', marker = 'o', linewidth=2, markersize=12, markerfacecolor= 'm',
         markeredgecolor= 'm')
plt.title('WCSS vs Number of Cluster')
plt.xlabel('Number of Cluster')
plt.ylabel('WCSS')
plt.xticks(list(range(1,11)))
plt.show()

#Elbow Method with yellowbrick library
visualizer = KElbowVisualizer(model1, k=(2,10))
visualizer.fit(X_std)
visualizer.show()

K = range(2,8)
fits=[]
score=[]

for k in K:
    model = KMeans(n_clusters = k, random_state = 0, n_init= 'auto').fit(X_std)
    fits.append(model)
    score.append(silhouette_score(X_std, model.labels_, metric='euclidean'))

sns.lineplot(x = K, y = score)

"""Cluster terbaik (k) terdapat pada 4 cluster"""

# Kmeans n_cluster = 4
# Clustering Kmeans
kmeans_4 = KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=100)
kmeans_4.fit(X_std)

"""In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
"""

# Masukin cluster ke dataset
df_cluster['cluster'] = kmeans_4.labels_
df_cluster.head()

plt.figure(figsize=(6,6))
sns.pairplot(data=df_cluster,hue='cluster',palette='Set1')
plt.show()

df_cluster['CustomerID'] = df_preclust['CustomerID']
df_cluster_mean = df_cluster.groupby('cluster').agg({'CustomerID':'count','TransactionID':'mean','Qty':'mean','TotalAmount':'mean'})
df_cluster_mean.sort_values('CustomerID', ascending = False)

"""# **Kesimpulan**


# **Cluster 0**

1.   Memiliki jumlah pelanggan paling sedikit
2.   Memiliki pelanggan dengan nilai tertinggi pada setiap metriksnya

Rekomendasi :
- Melakukan survei kepuasan pelanggan
- Menawarkan promosi loyalitas untuk mempertahankan transaksi


# **Cluster 1**

1.   Karakteristik pelanggan dengan nilai terendah pada setiap metriksnya

Rekomendasi :
- Memberikan diskon produk untuk meningkatkan transaksi
- Melakukan survei untuk mengetahui pengembangan produk

# **Cluster 2**

1.   Karakteristik pelanggan dengan nilai kedua tertinggi pada setiap metriksnya

Rekomendasi :
- Melakukan upselling produk
- Memberikan promo untuk meningkatkan transaksi

# **Cluster 3**

1.   Memilik jumlah pelanggan terbanyak diantara cluster lain
2.   Karakteristik pelanggan dengan nilai ketiga tertinggi pada setiap metriksnya

Rekomendasi :
- Meningkatkan hubungan baik dengan pelanggan
- Memberikan apresiasi terutama kepada pelanggan setia
- Memberikan atau melakukan survei untuk memahami minat dari pelanggan







"""